import h5py
import pyarrow
import pyarrow.parquet
from openff.units import unit
from collections import defaultdict
import deepchem as dc
import typing

# setup the parquet datasets using the splits generated by deepchem



# load up both files
training_db = h5py.File("TrainingSet-v1.hdf5", "r")
valid_test_db = h5py.File('ValSet-v1.hdf5', 'r')

def create_parquet_dataset(parquet_name: str, deep_chem_dataset: dc.data.DiskDataset, reference_datasets: typing.List[h5py.File]):
    dataset_keys = deep_chem_dataset.X
    dataset_smiles = deep_chem_dataset.ids
    coloumn_names = ["smiles", "conformation", "dipole", "mbis-charges"]
    results = defaultdict(list)
    # keep track of the number of total entries, this is each conformation expanded as a unique training point
    total_records = 0
    for key, smiles in zip(dataset_keys, dataset_smiles):
        for dataset in reference_datasets:
            if key in dataset:
                data_group = dataset[key]
                group_smiles = data_group["smiles"].asstr()[0]
                assert group_smiles == smiles
                charges = data_group["mbis-charges"][()]
                dipoles = data_group["dipole"][()]
                conformations = data_group["conformations"][()] * unit.angstrom
                # workout how many entries we have
                n_records = charges.shape[0]
                total_records += n_records
                for i in range(n_records):

                    results["smiles"].append(smiles)
                    results["mbis-charges"].append(charges[i])
                    results["dipole"].append(dipoles[i])
                    # make to store in bohr
                    results["conformation"].append(conformations[i].m_as(unit.bohr).flatten())


    for key, values in results.items():
        assert len(values) == total_records, print(key)
    columns = [results[label] for label in coloumn_names]

    table = pyarrow.table(columns, coloumn_names)
    pyarrow.parquet.write_table(table, parquet_name)


for file_name, dataset_name in [('training.parquet', 'maxmin-train'), ('validation.parquet', 'maxmin-valid'), ('testing.parquet', 'maxmin-test')]:
    print('creating parquet for ', dataset_name)
    dc_dataset = dc.data.DiskDataset(dataset_name)
    create_parquet_dataset(parquet_name=file_name, deep_chem_dataset=dc_dataset, reference_datasets=[training_db, valid_test_db])

training_db.close()
valid_test_db.close()